{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORCaFbkpsNMd",
        "outputId": "346fd8af-38c7-4349-cdbc-4fd404d8f651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from torchvision.models import resnet18\n",
        "from torch import nn, save, max, no_grad, randperm\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "import torchvision.transforms as transforms\n",
        "from google.colab import drive\n",
        "from numpy import mean\n",
        "import pandas as pd\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip /content/drive/MyDrive/model_comparisons_for_baboon/IMAGES_BABOON_ARI.zip -d /content/drive/MyDrive/model_comparisons_for_baboon/IMAGES_BABOON_ARI\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k57rg-tQ2hb",
        "outputId": "d1a11424-4682-48ea-f82c-c87f7569da36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/drive/MyDrive/model_comparisons_for_baboon/IMAGES_BABOON_ARI.zip, /content/drive/MyDrive/model_comparisons_for_baboon/IMAGES_BABOON_ARI.zip.zip or /content/drive/MyDrive/model_comparisons_for_baboon/IMAGES_BABOON_ARI.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZhCJFNmBXr6-"
      },
      "outputs": [],
      "source": [
        "def load_train_data(folder = \"100\", N_train_samples = 100):\n",
        "  transform = transforms.Compose([\n",
        "      transforms.Grayscale(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "  ################# change folder here when changing the task ######################\n",
        "  train_ds = ImageFolder(root = '/content/drive/MyDrive/model_comparisons_for_baboon/IMAGES_FOR_BABOON/Dataset_ari_steps1k/task_w_nw/'+folder+'/train', transform=transform)\n",
        "  subsample_train_indices = randperm(len(train_ds))[:N_train_samples]\n",
        "  train_dl = DataLoader(train_ds, batch_size=64, sampler=SubsetRandomSampler(subsample_train_indices))\n",
        "  return train_dl\n",
        "\n",
        "def load_test_data(folder = \"100\"):\n",
        "  transform = transforms.Compose([\n",
        "      transforms.Grayscale(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "  ################# change folder here when changing the task ######################\n",
        "  test_ds = ImageFolder(root='/content/drive/MyDrive/model_comparisons_for_baboon/IMAGES_FOR_BABOON/Dataset_ari_steps1k/task_w_nw/'+folder+'/train', transform=transform)\n",
        "  test_dl = DataLoader(test_ds, batch_size=64)\n",
        "  return test_ds\n",
        "\n",
        "def fit_test_model(folder = \"100\", N_train_samples = 100):\n",
        "\n",
        "  train_dl = load_train_data(folder = folder, N_train_samples = N_train_samples)\n",
        "  test_dl = load_train_data(folder = folder)\n",
        "\n",
        "  model = resnet18(num_classes=2)\n",
        "  model.conv1 = nn.Conv2d(1, 64, kernel_size=(63, 63), stride=(2, 2), padding=1, bias=False)\n",
        "\n",
        "  # Define the optimizer and loss function\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  #print(train_dl)\n",
        "  for _ in range(15):\n",
        "      for inputs, labels in train_dl:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  model.eval()\n",
        "  with no_grad(): # This is optional but saves memory\n",
        "    for inputs, labels in test_dl:\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        # Calculate predictions\n",
        "        _, predicted = max(outputs, 1)\n",
        "        # Calculate accuracy (optional)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "  test_acc = correct / total\n",
        "\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  model.eval()\n",
        "  with no_grad(): # This is optional but saves memory\n",
        "    for inputs, labels in train_dl:\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        # Calculate predictions\n",
        "        _, predicted = max(outputs, 1)\n",
        "        # Calculate accuracy (optional)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "  train_acc = correct / total\n",
        "\n",
        "  return ({'folder': folder, 'samples': N_train_samples, 'train': train_acc, 'test': test_acc})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "32UtPc-sZoSm"
      },
      "outputs": [],
      "source": [
        "def wrapper_model_fit(folder_list = [\"100\",\"200\",\"300\"], N_train_samples_list = [1240, 2479, 3719, 4958], iterations = 10):\n",
        "  data = []\n",
        "\n",
        "  for folder in folder_list:\n",
        "    for N_train_samples in N_train_samples_list:\n",
        "      print(str(N_train_samples)+\" \"+folder)\n",
        "      acc_list = [fit_test_model(folder = folder, N_train_samples = N_train_samples) for i in range(iterations)]\n",
        "      print(mean(acc_list))\n",
        "      [data.append({'folder': folder, 'train_samples': N_train_samples, 'acc': acc}) for acc in acc_list]\n",
        "\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_test_data(folder = \"100\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzrjhIaJv2t4",
        "outputId": "b00c6b7c-6bbf-418c-efc1-2bb6b6094111"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset ImageFolder\n",
              "    Number of datapoints: 99\n",
              "    Root location: /content/drive/MyDrive/model_comparisons_for_baboon/IMAGES_FOR_BABOON/Dataset_ari_steps1k/task_w_nw/100/train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Grayscale(num_output_channels=1)\n",
              "               ToTensor()\n",
              "               Normalize(mean=(0.5,), std=(0.5,))\n",
              "           )"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "5KU6OI5Zbe4S",
        "outputId": "14b4b293-3f42-4acf-b5e6-e840af03044d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-255e5bbe4e40>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#[round(N*.1), round(N*.25), round(N*.5), round(N*.75), round(N*1)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#data = wrapper_model_fit(folder_list = [\"normal\",\"original_oPE\",\"50p_oPE\"], N_train_samples_list = [round(N*0.5)], iterations = 10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_model_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"100\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_train_samples_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-93ba7b52f8b3>\u001b[0m in \u001b[0;36mwrapper_model_fit\u001b[0;34m(folder_list, N_train_samples_list, iterations)\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_train_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m       \u001b[0macc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfit_test_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_train_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN_train_samples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'folder'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_samples'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mN_train_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'acc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0macc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0macc_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3430\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3432\u001b[0;31m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0m\u001b[1;32m   3433\u001b[0m                           out=out, **kwargs)\n\u001b[1;32m   3434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         ret = um.true_divide(\n",
            "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'dict' and 'dict'"
          ]
        }
      ],
      "source": [
        "N = 100\n",
        "#[round(N*.1), round(N*.25), round(N*.5), round(N*.75), round(N*1)]\n",
        "#data = wrapper_model_fit(folder_list = [\"normal\",\"original_oPE\",\"50p_oPE\"], N_train_samples_list = [round(N*0.5)], iterations = 10)\n",
        "data = wrapper_model_fit(folder_list = [\"100\"], N_train_samples_list = [round(N)], iterations = 10)\n",
        "df = pd.DataFrame.from_records(data)\n",
        "df\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}